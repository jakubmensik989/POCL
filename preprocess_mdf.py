import pandas as pd
import numpy as np
import pickle
import os
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from datetime import datetime, timedelta

# ================= é…ç½®åŒº =================
DATA_OUT_DIR = "./Dataset/"
if not os.path.exists(DATA_OUT_DIR):
    os.makedirs(DATA_OUT_DIR)

SAMPLE_MEMBER_COUNT = 5000 

print("Step 1: æ­£åœ¨å°è¯•è¯»å–æ•°æ® (è‡ªåŠ¨å¤„ç†ç¼–ç )...")

# ğŸ› ï¸ ä¿®å¤ç‚¹ 1: å°è¯•å¤šç§ç¼–ç æ ¼å¼ï¼Œé˜²æ­¢å› ä¸­æ–‡å¯¼è‡´çš„æŠ¥é”™
def read_csv_safe(path):
    encodings = ['utf-8', 'gbk', 'gb18030', 'ISO-8859-1']
    for enc in encodings:
        try:
            df = pd.read_csv(path, encoding=enc, dtype={'ProviderID': str, 'Vendor': str})
            print(f"âœ… æˆåŠŸä½¿ç”¨ {enc} ç¼–ç è¯»å– {path}")
            return df
        except UnicodeDecodeError:
            continue
    raise ValueError(f"âŒ æ— æ³•è¯»å– {path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ï¼")

try:
    claims = read_csv_safe('Claims_ovr.csv')
    targets = read_csv_safe('DaysInHospital_Y2.csv')
except FileNotFoundError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ï¼è¯·ç¡®è®¤ Claims.csv å’Œ DaysInHospital_Y2.csv å°±åœ¨å½“å‰æ–‡ä»¶å¤¹é‡Œã€‚")
    exit()

# ğŸ› ï¸ ä¿®å¤ç‚¹ 2: æ¸…é™¤åˆ—åé‡Œçš„ç©ºæ ¼ï¼Œé˜²æ­¢ KeyError
claims.columns = claims.columns.str.strip()
targets.columns = targets.columns.str.strip()
print("âœ… åˆ—åæ¸…æ´—å®Œæˆ")

# é‡‡æ ·é€»è¾‘
unique_members = claims['MemberID'].unique()
if len(unique_members) > SAMPLE_MEMBER_COUNT:
    print(f"âš ï¸ æ•°æ®é‡‡æ ·: å–å‰ {SAMPLE_MEMBER_COUNT} ä¸ªç—…äºº...")
    sampled_members = unique_members[:SAMPLE_MEMBER_COUNT]
    claims = claims[claims['MemberID'].isin(sampled_members)]
    targets = targets[targets['MemberID'].isin(sampled_members)]

# æ ‡ç­¾åˆå¹¶
targets['Label'] = (targets['DaysInHospital'] > 0).astype(int)
data = pd.merge(claims, targets[['MemberID', 'Label']], on='MemberID', how='left')
data['Label'] = data['Label'].fillna(0).astype(int)

# ğŸ› ï¸ ä¿®å¤ç‚¹ 3: å¼ºå£®çš„æ—¥æœŸè§£æé€»è¾‘ (å¤„ç† '1æœˆ2æ—¥' å’Œ Excel ä¹±ç )
print("Step 2: æ­£åœ¨è§£ææ—¥æœŸ...")
def robust_parse_date(row):
    # å¹´ä»½
    y_str = str(row['Year']).strip()
    base_year = 2009
    if y_str == 'Y2': base_year = 2010
    elif y_str == 'Y3': base_year = 2011
    
    # æœˆä»½ DSFS
    dsfs = str(row['DSFS(Days Since First Service)'])
    month_offset = 0
    
    if 'month' in dsfs:
        try:
            # å¤„ç† "8- 9 months"
            month_offset = int(dsfs.split('-')[0].strip())
        except:
            pass
    
    return datetime(base_year, 1, 1) + timedelta(days=month_offset*30)

data['ClaimStartDt'] = data.apply(robust_parse_date, axis=1)
data = data.sort_values('ClaimStartDt').reset_index(drop=True)

# ç‰¹å¾å·¥ç¨‹
print("Step 3: æ­£åœ¨å¤„ç†ç‰¹å¾...")
data['ProviderID'] = data['ProviderID'].fillna('Unknown')
feature_cols = ['Specialty', 'PlaceSvc', 'PrimaryConditionGroup', 'ProcedureGroup', 'CharlsonIndex', 'LengthOfStay']

for col in feature_cols:
    # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ '1æœˆ2æ—¥' è¿™ç§ä¹±å…¥çš„æ•°æ®å¯¼è‡´æŠ¥é”™
    data[col] = data[col].fillna('Unknown').astype(str)
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

scaler = MinMaxScaler()
feature_data = scaler.fit_transform(data[feature_cols].values)

# å»ºå›¾
print("Step 4: æ­£åœ¨æ„å»ºå›¾ç»“æ„...")
def create_edges_fast(df_subset):
    edges = []
    # ä»…è¿æ¥åŒä¸€ä¸ªç—…äººçš„æ—¶åºè®°å½•
    grp_member = df_subset.groupby('MemberID').indices
    for indices in grp_member.values():
        if len(indices) > 1:
            src = indices[:-1]
            dst = indices[1:]
            edges.extend(zip(src, dst))
            edges.extend(zip(dst, src))
            
    # ä»…è¿æ¥åŒä¸€ä¸ªåŒ»ç”Ÿçš„ç›¸é‚»è®°å½• (é™åˆ¶æ•°é‡é˜²æ­¢å¡æ­»)
    grp_provider = df_subset.groupby('ProviderID').indices
    for pid, indices in grp_provider.items():
        if pid == 'Unknown': continue
        if len(indices) > 1:
            limit_indices = indices[:20] 
            src = limit_indices[:-1]
            dst = limit_indices[1:]
            edges.extend(zip(src, dst))
            edges.extend(zip(dst, src))

    if not edges: return np.array([[], []])
    return np.array(list(set(edges))).T

# è¾“å‡º
print("Step 5: æ­£åœ¨ä¿å­˜...")
data['YearMonth'] = data['ClaimStartDt'].apply(lambda x: x.strftime('%Y-%m'))
time_steps = sorted(data['YearMonth'].unique())

dataset_online = []
dataset_two = []

for t_step in time_steps:
    step_df = data[data['YearMonth'] == t_step]
    if step_df.empty: continue
    
    curr_feats = feature_data[step_df.index]
    curr_labels = step_df['Label'].values.reshape(-1, 1)
    combined_matrix = np.hstack([curr_feats, curr_labels])
    
    edge_index = create_edges_fast(step_df.reset_index(drop=True))
    dataset_online.append((combined_matrix, edge_index))
    dataset_two.append((combined_matrix, edge_index))

with open(DATA_OUT_DIR + "datasetonline.dat", "wb") as f:
    pickle.dump(dataset_online, f)
with open(DATA_OUT_DIR + "datasettwo.dat", "wb") as f:
    pickle.dump(dataset_two, f)

print("âœ… æˆåŠŸè·‘é€šï¼")

# ==========================================
# æ–°å¢åŠŸèƒ½ï¼šå¯¼å‡ºå¤„ç†åçš„æ•°æ®ä¸º CSV
# ==========================================
print("Step 6: æ­£åœ¨å¯¼å‡ºå¯è§†åŒ– CSV æ–‡ä»¶...")

# 1. å¯¼å‡ºã€èŠ‚ç‚¹ç‰¹å¾è¡¨ã€‘ (Node Features)
# è¿™å¼ è¡¨åŒ…å«äº†æ¨¡å‹çœŸæ­£ä½¿ç”¨çš„æ‰€æœ‰æ•°æ®ï¼šå¯¹é½åçš„æ—¥æœŸã€æ•°å­—åŒ–çš„ç‰¹å¾ã€æ ‡ç­¾
export_df = data.copy()
# åªä¿ç•™æ ¸å¿ƒåˆ—å’Œç‰¹å¾åˆ—
save_cols = ['MemberID', 'ProviderID', 'ClaimStartDt', 'Label'] + feature_cols
# ä¿å­˜
export_df[save_cols].to_csv(DATA_OUT_DIR + 'processed_node_features.csv', index=False)
print(f"âœ… èŠ‚ç‚¹ç‰¹å¾è¡¨å·²ä¿å­˜: {DATA_OUT_DIR}processed_node_features.csv")

# 2. å¯¼å‡ºã€è¾¹åˆ—è¡¨ã€‘ (Edge List)
# åªå¯¼å‡ºå‰ 10000 æ¡è¾¹
# (æ³¨æ„ï¼šå› ä¸ºä¹‹å‰çš„ dataset_online æ˜¯åˆ†æ—¶é—´æ­¥å­˜çš„ï¼Œæˆ‘ä»¬è¿™é‡Œé‡æ–°ç”Ÿæˆä¸€ä¸ªå…¨é‡çš„è¾¹åˆ—è¡¨ç”¨äºå±•ç¤º)
print("æ­£åœ¨ç”Ÿæˆè¾¹åˆ—è¡¨ CSV (ä»…å–å‰ 10000 æ¡ç¤ºä¾‹)...")
all_edges = create_edges_fast(data.head(5000))  # å¯¹å‰ 5000 è¡Œæ•°æ®å»ºå›¾ä½œä¸ºç¤ºä¾‹
if all_edges.shape[1] > 0:
    edge_df = pd.DataFrame(all_edges.T, columns=['Source_Node_Index', 'Target_Node_Index'])
    # æ˜ å°„å›çœŸå®çš„ MemberID (å¯é€‰ï¼Œæ–¹ä¾¿ç†è§£)
    # æ³¨æ„ï¼šè¿™é‡Œçš„ Index æ˜¯ç›¸å¯¹äº head(5000) çš„ç´¢å¼•
    edge_df['Source_MemberID'] = data.iloc[edge_df['Source_Node_Index']]['MemberID'].values
    edge_df['Target_MemberID'] = data.iloc[edge_df['Target_Node_Index']]['MemberID'].values

    edge_df.to_csv(DATA_OUT_DIR + 'processed_edge_list.csv', index=False)
    print(f"âœ… è¾¹åˆ—è¡¨å·²ä¿å­˜: {DATA_OUT_DIR}processed_edge_list.csv")
else:
    print("âš ï¸ è­¦å‘Šï¼šå‰ 5000 æ¡æ•°æ®æ²¡æœ‰ç”Ÿæˆä»»ä½•è¾¹ï¼Œå¯èƒ½æ•°æ®å¤ªç¨€ç–ã€‚")

print("=" * 30)